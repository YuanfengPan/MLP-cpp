# 项目链接https://github.com/YuanfengPan/MLP-cpp（持续完善中）

# C++ 实现的前馈神经网络（MNIST 分类）——完整说明文档

**项目概览：** 本程序使用 C++ 实现了一个简易的前馈神经网络（Feedforward Neural Network, FNN）用于 MNIST 手写数字分类。该网络由输入层、多个隐藏层和输出层组成，信息单向流动。程序首先利用 `MNIST` 类加载训练数据，并将数据分批送入网络进行训练（**前向传播 + 反向传播**）；训练完成后保存模型参数，再加载测试数据逐一前向推断并统计准确率。整个模型结构类似典型的三层网络（输入→隐藏→输出），层与层之间通过权重连接并加上激活函数非线性变换。下图展示了一个简单三层前馈神经网络的示意图，其中数据从左侧输入层依次经过隐藏层，最后输出分类结果。

图：典型的前馈神经网络结构示意图，信息从左至右单向流动（输入层→隐藏层→输出层）。该程序即采用类似的多层结构，对输入手写数字图像进行分层线性变换和非线性激活，再由 Softmax 输出概率分布，实现分类。

## Matrix 模板类

程序定义了模板类 `matrix<T>` 来存储和操作矩阵数据，实现了常见的矩阵运算，为各层计算提供基础支持。主要功能包括：

* **构造与析构**：可通过默认构造或指定尺寸（行 × 列 和填充值）创建矩阵，内部用一维数组按行主序连续存储，有利于提高数据访问效率。支持拷贝构造和移动构造（右值引用），移动语义可显著提升性能。
* **加减法运算**：重载了矩阵与标量或矩阵之间的加法/减法（`+`、`-`、`+=`、`-=`），逐元素对应计算，若维度不匹配则报错提示。
* **乘法运算**：支持矩阵与标量乘法（`*`、`*=`），以及两个矩阵相乘（`*`、`*=`），要求左矩阵列数等于右矩阵行数。实现时使用三层循环依次累加乘积，结果存入新矩阵。
* **逐元素乘积（Hadamard）**：通过 `operator^` 重载实现两个同型矩阵逐元素相乘（Hadamard 乘积），逐元素相乘并返回新矩阵。
* **转置**：`transpose()` 返回行列互换的新矩阵。
* **元素映射**：`func(F f)` 接受一元函数 `f`，对矩阵每个元素应用该函数并返回新矩阵；同理 `func(F f, const matrix& m)` 对应二元函数。该机制用于实现激活函数的逐元素计算。
* **展平和扩展**：`flatten()` 将矩阵展平为列向量（如 `[row,col]→[row*col,1]`）；`extend_col(n)` 和 `extend_row(n)` 可将列向量或行向量沿新维度复制扩展，用于在批处理时批量复制偏置或标签等。
* **行列求和与最大值**：`sum()` 计算所有元素之和；`sum_row()` 返回每行元素之和（`[row,1]`）；`sum_col()` 返回每列之和（`[1,col]`）；`max_col()` 返回每列的最大值（`[1,col]`），这些常用于计算偏置梯度、归一化或稳定性操作。
* **下标和输出**：重载 `operator[]` 支持 `mat[i][j]` 访问矩阵元素；`operator<<` 将矩阵内容输出到流，用于调试打印。

这些矩阵操作为线性层和激活层等的计算提供了基础。例如，全连接层的前向计算需要矩阵乘法，Softmax 需要指数和归一化运算，Loss 层需要对矩阵元素求和或取对数等都依赖于这些操作。值得注意的是，底层采用一维连续数组存储矩阵，使得数据访问更高效，对神经网络这类计算密集的程序性能很重要。

pyh ps：
其实我是可以实现类似于numpy广播机制的基本运算的，但是在神经网络的训练与实现中并不会实现太过复杂的广播机制，所以将其显式化地写作两个函数 `extend_col`和 `extend_row`，以配合批次化训练的需求

以及这个矩阵类底层用一维数组实现，能够显著减少它指针、地址的跳转和运算，连续的内存排布能够大大提高数据访问的速度。特别对于神经网络这种计算速度和运行效率十分重要的代码来说。

以及移动语义和右值引用的实现对程序运行效率的提高是有显著影响的。大概是一次完整train+test的时间会从30min->22min
关于左右值：在实现移动语义和右值引用时进行学习才知道const T&是既可以接收左值也可以接收右值的 但是T&& 只能接收右值 并且移动语义是会对T&&本身产生改变的（所以不能也不需要写作const T&&）

## 全局参数类

类 `Parameter` 定义了一组神经网络超参数作为静态成员，包括训练轮数 `max_epoch`、批处理大小 `batch_size`、学习率 `lr`，以及 L1 正则项系数 `wd1` 和 L2 正则项系数 `wd2`。代码在末尾给出了默认值：`max_epoch=1`、`batch_size=100`、`lr=0.002`、`wd1=0`、`wd2=2e-6`。集中管理超参数可以方便地对训练过程进行配置。实际训练时，这些参数会影响梯度下降的更新幅度和正则化强度，从而影响收敛速度和最终精度。

## 网络层（Node）

程序设计了基类 `node` 来定义网络层的接口，每个层派生类必须实现前向计算 `forward()` 和反向传播 `backward()` 两个虚函数，以及参数保存/加载和正则化损失计算的接口。主要的层包括：

* **Linear（全连接层）**：构造时指定输入维度 `in_row` 和输出维度 `out_row`，并随机初始化权重矩阵 `weight[in_row,out_row]` 和偏置向量 `bias[out_row,1]`。

  * **前向（Forward）**：给定输入矩阵 `input`（尺寸 `[in_row,batch]`），计算线性变换 $W^T x + b$，其中 `weight.transpose() * input + bias.extend_col(batch)` 实现了对每个批次样本的加法。输出矩阵尺寸为 `[out_row,batch]`。
  * **反向（Backward）**：输入为上一层梯度 `grad_output`（尺寸 `[out_row,batch]`）。先计算梯度并更新参数：偏置梯度为 `grad_output` 按列求和（`sum_row()` 返回 `[out_row,1]`），学习率缩放后更新 `bias`；权重梯度为输入 `last_input` 与 `grad_output^T` 的乘积，再加上 L1/L2 正则项梯度（代码注释部分展示了 L1、L2 正则的计算方式）。更新完成后，返回“传递给上一层”的梯度：即 `weight * grad_output`（尺寸 `[in_row,batch]`）。注意这里使用的是普通梯度下降（没有对批大小做除法归一化，未使用优化算法如动量或自适应学习率）。
  * **参数持久化**：`save_parameters` 和 `load_parameters` 将权重与偏置按二进制写入或读出，用于模型的存储与加载。
  * **正则化损失**：提供了计算当前权重 L1/L2 范数的方法，用于汇总正则化损失。
* **ReLU（修正线性单元激活）**：前向函数对输入中的每个元素执行 $\mathrm{ReLU}(x)=\max(0,x)$，即小于0的值置零；反向传播中，只有对应输入大于0的位置保留梯度，其余位置梯度置零（`grad_output.func((x,y)->(y>0?x:0), last_input)` 实现）。ReLU 简单高效，常用于隐藏层，可引入非线性。
* **Sigmoid（S 型激活）**：前向函数对每个元素计算 $\sigma(x) = 1/(1+e^{-x})$，将输入压缩到 (0,1) 区间；反向传播使用导数 $\sigma'(x)=\sigma(x)(1-\sigma(x))$（代码通过二元函数 `dsigmoid(x) = sigmoid(x)*sigmoid(-x)` 计算，其实对于 logistic 函数 \$\sigma(-x)=1-\sigma(x)\$，与 \$\sigma(x)(1-\sigma(x))\$ 等价）。Sigmoid 在输出层常用于二分类，但本网络主要用于隐藏层的非线性映射（本代码架构中并未实际使用 Sigmoid，只有实现备用）。
* **Softmax（归一化指数层）**：通常放在输出层，将实数输出转化为概率分布。前向计算时，为了数值稳定先对每列（样本）的所有输入减去其最大值（`input.max_col()`），再取指数并除以指数和（`sum_col()`），确保输出在 (0,1) 且每列和为 1。代码中对可能出现的 0、NaN、无穷进行检查，避免除零异常。这样得到的 `last_input` 即各类别概率。

  * **反向传播**：一般而言，Softmax 的反向传播需要涉及其雅可比矩阵。但在实践中，当 Softmax 与交叉熵损失结合使用时，其梯度可简化。代码中 `Softmax::backward` 直接返回传入的梯度，不做变换，这是因为交叉熵层已经将 Softmax 的求导效果包含进去。
* **CrossEntropy（交叉熵损失层）**：用于多分类损失计算。内部存储一批真实标签 `label` 向量。

  * **前向**：假设输入 `input` 是 Softmax 的概率输出（尺寸 `[num_classes, batch]`），交叉熵按公式 $\mathrm{Loss} = -\frac{1}{N} \sum_{i=0}^{N-1} \log(p_{y_i,i})$ 计算，其中 \$y\_i\$ 是第 \$i\$ 个样本的真实类别下标。代码通过累加 \$-\log(\text{prob})\$ 并除以样本数得到平均损失。交叉熵衡量预测概率分布与真实分布的差异；当正确类别的预测概率越接近1，损失越低。
  * **反向**：交叉熵对输入 logits 的梯度有简洁形式：$\frac{\partial L}{\partial o_{ij}} = p_{ij} - y_{ij}$，其中 \$p\_{ij}\$ 是 Softmax 后的概率，\$y\_{ij}\$ 是真实标签的 one-hot 向量。代码中先令 `res = last_input`（Softmax 概率矩阵），然后对每个样本的真实类别位置减 1，再整体乘以 \$1/N\$。得到的 `res` 就是 `[num_classes,batch]` 的梯度矩阵，被传回上一层。这个结果与分析得到的一致。因此实现时无需显式计算交叉熵关于 Softmax 的雅可比，直接用这一简单公式即可。

**节点层次结构示例：** 本程序的 `Graph` 类将各层按序连接：输入层（784 维）→第1全连接层（500 或 512 维）→ReLU→第2全连接层（500/512 维）→ReLU→第3全连接层（...继续）→ReLU→输出层（10 维）→Softmax→交叉熵。信息依次向前传播完成预测后，再从输出层依次向前传播回去更新每层参数。这样的网络结构使得每层只负责自己的计算和梯度更新，符合前馈神经网络的定义。
pyh ps:为什么save_parameter和load_parameter都要使用虚函数呢？一个原因是在一个神经网络中层的数量极其有限，在虚函数表的开销上并没有非常显著的影响。另外虽然并非每一种神经网络的结构都有参数可供训练和保存，但是将其写作统一的结构在Graph层面上更方便于统一操作。

## 网络结构（Graph 类）

`Graph` 类组织并运行整个网络结构。构造函数中分配了一个层指针数组 `nodes`，实例化并按顺序存放各层对象：例如 `Linear(784,512)`、`Relu()`、`Linear(512,256)`、`Relu()`… 直到最后的 `Softmax` 和 `CrossEntropy` 层。通过这种方式，网络层数和结构固定，前向/反向计算易于统一管理。`Graph::run(input, label)` 方法实现一次完整的前向和反向传播：

1. **前向传播**：依次调用每一层的 `forward(res)`，其中 `res` 最初为输入矩阵（尺寸 `[784,batch]`）。经过多个线性变换和激活函数后，最后由 Softmax 输出 `[10,batch]` 的概率矩阵。交叉熵层的 `forward` 则使用该概率和真实标签计算并记录损失（不改变返回矩阵）。
2. **反向传播**：反向依次调用各层的 `backward(grad)`，从交叉熵层开始，其 `backward` 返回梯度（\$p - y\$）并乘以 \$1/N\$；然后梯度传递到 Softmax 层（Softmax 的 `backward` 直接透传梯度），再到每个上游的激活或线性层，最终完成整个网络的梯度计算并更新每层可训练参数（线性层中的权重和偏置）。

`Graph` 类还支持参数持久化操作：`save_parameters(path)` 将所有层的参数写入二进制文件，`load_parameters(path)` 则可从文件恢复参数，使得训练好的模型可以重用。其 `test(input,label)` 函数用于单样本预测：仅执行前向传播，不做梯度更新，选取最大概率的类别与真实标签比较来判断预测是否正确。最终，程序会统计在测试集上预测正确的样本数，计算准确率。
pyh ps:为什么Softmax 和 CrossEntropy要分开实现呢？在很多很多的神经网络结构中他们是合并实现的，能够提高计算速度。但是我还是喜欢分开实现，提高这些节点代码本身的泛用性。
这样的结构是一个经典的三层前馈神经网络（**输入层 → 隐藏层 → 输出层**），信息在层间单向传递。
。

## MNIST 数据加载与训练流程

`MNIST` 类负责读取 MNIST 数据、训练网络和测试评估。其流程如下：

* **加载训练数据**：`loadtraindata()` 打开 MNIST 的图像（`train-images.idx3-ubyte`）和标签（`train-labels.idx1-ubyte`）文件。MNIST 的 IDX 格式数据为大端存储，需要按字节顺序读取整数头信息（图片数量、尺寸等）。每读入一个批次（默认 100 张图），将像素灰度归一化到 \[0,1] 并存入一个 `matrix<double>`（尺寸 `[784, batch_size]`），标签读取到一个 `vector<int>`。批次化处理后，共生成 `60000/100=600` 个批次数据。读取过程中每隔若干批次打印进度提示。
* **训练**：`train()` 函数按 `max_epoch`（默认为1）进行轮次训练。每轮遍历所有批次，对第 `i` 个批次调用 `graph.run(train_data[i], train_label[i])` 完成一次前向和反向传播。每隔 50 个批次打印进度。这一步利用梯度下降更新网络参数。训练结束后，调用 `graph.save_parameters("mnist4.bin")` 保存最终模型参数。
* **加载测试数据**：`loadtestdata()` 类似地读取测试图像和标签文件，将每张图分别存为 `[784,1]` 的矩阵并归一化，标签存为单元素向量。程序打印测试集大小（10000张）。
* **测试评估**：`run()` 最后遍历测试集每个样本，对 `graph.test(test_data[i], test_label[i])`，判断预测结果是否与真实标签一致，并统计正确数目。输出格式为 “`cnt/10000 correct`” 表示准确率。

此外，`MNIST` 类提供了对外接口：可以通过 `load_displaydata("image.bmp")` 加载一张 28×28 位图（BMP）并转换为与 MNIST 风格一致的 `[784,1]` 输入 `display_image`，然后调用 `predict_display_data()` 利用已加载的模型输出预测数字。此功能方便用户用自己的手写图像进行测试。示例使用见下段“**使用示例**”。

## 使用示例

* **编译与运行：** 将代码文件编译，例如 `g++ main.cpp -o mlp`。直接运行可开始训练和测试流程：`./mlp`。程序会自动加载训练数据（若已保存模型则加载已有参数）、训练一轮、保存模型并在测试集上评估准确率。
* **加载已有模型：** 若之前已训练过模型，并将参数保存在文件（如 `"mnist4.bin"`），再次运行时可以跳过重新训练，只加载参数后进入测试。
* **预测单个图像：** 假设有一张符合要求的 28×28 灰度 BMP 图像 `digit.bmp`，可以使用以下代码进行预测：

  ```cpp
  MNIST mnist;
  mnist.graph.load_parameters("mnist4.bin");  // 加载模型参数
  mnist.load_displaydata("digit.bmp");        // 加载并预处理图像
  int pred = mnist.graph.predict(mnist.display_image);
  cout << "Predicted digit: " << pred << endl;
  ```

  其中 `predict` 仅进行前向传播并返回概率最大的类别。程序也提供了 `predict_display_data()`，内部做了同样的事并直接打印结果。

## 训练性能与效果

* **训练速度：** 该代码纯 C++ 实现，使用连续内存和移动语义后（右值引用）效率提升明显。据作者测试，从无优化的版本到使用移动构造等特性，完整训练+测试时间从约 30 分钟降至 22 分钟。批次化（`batch_size=100`）训练进一步提高了效率，相当于每次更新利用 100 个样本的平均梯度。
* **准确率：** 默认设置下一次迭代（1 轮）后，测试集准确率约在 85–90% 左右。通过调整学习率、增加训练轮次和隐藏层神经元数，可进一步提升准确率。例如，根据代码注释记录的调试结果，经过多轮训练和参数调整后，准确率可超过 93%（TensorFlow/Keras 简单模型经过更多训练可达 \~97%）。需要注意的是，完全连接网络在 MNIST 上通常可以达到 95% 以上的高准确率。训练过程中常见的规律是：损失曲线逐渐下降、准确率逐渐上升，这种训练曲线可用于判断模型收敛情况。该程序没有额外绘制曲线，但可根据损失值和准确率的输出变化推测训练效果。

## 优缺点分析

**优点：**

* *概念清晰：* 从零实现了神经网络的前向传播、反向传播和参数更新，帮助初学者理解基本流程。
* *无外部库依赖：* 所有运算均自定义完成，无需依赖其他数值库，展示了矩阵运算和神经网络原理的手动实现细节。
* *可扩展性：* 设计了基类 `node` 和 `Graph` 结构，便于添加新层或修改网络结构。参数保存加载使用二进制文件，便于模型复用。
* *性能优化：* 使用一维连续数组存储数据减少指针间跳转开销；采用 C++11 移动语义减少不必要的拷贝；批量梯度下降利用 CPU 向量化优势。

**缺点：**

* *功能有限：* 只实现了基础的全连接网络，无卷积或高级优化（如动量、Adam 等）。反向传播采用最简单的梯度下降，没有归一化处理也没有批量归一化、正则化细节（虽然代码提供了正则化接口，但在反向传播中默认未启用）。
* *代码通用性：* 数据路径硬编码在函数参数中（默认指向固定路径），跨平台通用性和错误处理不够完善。
* *效率限制：* Python 等高级框架（TensorFlow、PyTorch）使用 GPU 和高度优化的 BLAS 库，速度远优于此纯 CPU 实现。本程序仅供教学演示，不适用于大规模训练。
* *结果略低：* 简单网络结构和少量迭代导致最终准确率不及深度学习库示例高（例如 Keras 示例经 5 轮训练可达 \~97%）。改进可以考虑增大网络规模、多轮训练、学习率调整或使用更先进的优化器。

**小结：** 本代码端到端演示了前馈神经网络在 MNIST 数据集上的训练和测试：矩阵类提供了基础运算，线性层和激活层串联构成网络结构，通过 Softmax+交叉熵完成分类损失计算，反向传播更新参数。学习本实现有助于理解神经网络前向推理和反向传播的细节，明确 Softmax 输出概率和交叉熵损失的组合优势。在实际工程中，建议使用成熟深度学习框架以获得更高精度和效率，但该示例对教学和研究原理具有参考价值。

**参考文献：**

* FNN 结构与前馈特性；线性层和 ReLU、Sigmoid 定义；Softmax 和交叉熵原理；Keras MNIST 示例结果。
